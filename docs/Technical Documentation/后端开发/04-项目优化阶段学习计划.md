# 04-项目优化阶段学习计划

## 学习目标

通过本阶段学习，您将掌握Python后端项目的生产环境优化技能，包括性能监控、缓存机制、API文档生成和容器化部署等企业级开发必备技能。以人力资源调度系统为例，实现完整的生产环境部署和运维监控体系。

### 核心技能目标
- 实现Prometheus + Grafana性能监控体系
- 掌握Redis缓存策略和会话管理
- 配置Swagger/OpenAPI自动化文档生成
- 实现Docker容器化和多环境部署
- 掌握Nginx反向代理和负载均衡
- 理解CI/CD流水线和自动化部署
- 掌握日志聚合和错误追踪系统

## 实战项目：人力资源调度系统生产环境部署

我们将在前面阶段开发的系统基础上，添加完整的生产环境优化功能，包括：
- 性能监控和指标收集
- Redis缓存和会话存储
- API文档自动生成
- Docker容器化部署
- 负载均衡和高可用配置
- 自动化部署流水线

---

## 第一步：性能监控系统设计

### 1.1 Prometheus监控架构

**监控系统架构**：
```
应用服务 → Prometheus Metrics → Prometheus Server → Grafana Dashboard
    ↓              ↓                    ↓              ↓
业务指标      系统指标            数据存储        可视化展示
```

**监控目录结构**：
```
monitoring/
├── prometheus/
│   ├── prometheus.yml      # Prometheus配置
│   └── rules/
│       └── alerts.yml      # 告警规则
├── grafana/
│   ├── dashboards/         # 仪表板配置
│   └── provisioning/       # 自动配置
└── docker-compose.yml      # 监控服务编排
```

### 1.2 应用指标收集配置

**安装监控依赖**：
```txt
# 监控和指标
prometheus-client==0.17.1
prometheus-fastapi-instrumentator==6.1.0

# 性能分析
psutil==5.9.5
py-spy==0.3.14

# APM监控
elastic-apm==6.18.1
sentry-sdk[fastapi]==1.32.0
```

**FastAPI应用监控配置**：
```python
# app/utils/monitoring.py
"""
应用监控配置

集成Prometheus指标收集和性能监控。
"""
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_fastapi_instrumentator import Instrumentator
from fastapi import FastAPI, Request
import time
import psutil
import logging

# 自定义指标
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_USERS = Gauge(
    'active_users_total',
    'Number of active users'
)

DATABASE_CONNECTIONS = Gauge(
    'database_connections_active',
    'Active database connections'
)

SYSTEM_MEMORY_USAGE = Gauge(
    'system_memory_usage_bytes',
    'System memory usage in bytes'
)

class MonitoringManager:
    """
    监控管理器
    
    负责应用性能指标的收集和上报。
    """
    
    def __init__(self, app: FastAPI = None):
        self.app = app
        self.instrumentator = None
        if app is not None:
            self.init_app(app)
    
    def init_app(self, app: FastAPI):
        """
        初始化监控配置
        """
        # 启用Prometheus指标导出
        self.instrumentator = Instrumentator()
        self.instrumentator.instrument(app).expose(app)
        
        # 注册请求钩子
        app.before_request(self._before_request)
        app.after_request(self._after_request)
        
        # 注册系统指标收集
        self._setup_system_metrics()
        
        # 配置日志
        self._setup_logging(app)
    
    def _before_request(self):
        """
        请求开始前的处理
        """
        g.start_time = time.time()
    
    def _after_request(self, response):
        """
        请求结束后的处理
        """
        # 记录请求指标
        duration = time.time() - g.start_time
        
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.endpoint or 'unknown',
            status=response.status_code
        ).inc()
        
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=request.endpoint or 'unknown'
        ).observe(duration)
        
        return response
    
    def _setup_system_metrics(self):
        """
        设置系统指标收集
        """
        def collect_system_metrics():
            # 内存使用情况
            memory = psutil.virtual_memory()
            SYSTEM_MEMORY_USAGE.set(memory.used)
            
            # 数据库连接数（需要根据实际数据库实现）
            # DATABASE_CONNECTIONS.set(get_db_connection_count())
        
        # 定期收集系统指标
        import threading
        import time
        
        def metrics_collector():
            while True:
                try:
                    collect_system_metrics()
                    time.sleep(30)  # 每30秒收集一次
                except Exception as e:
                    logging.error(f'指标收集失败: {e}')
        
        thread = threading.Thread(target=metrics_collector, daemon=True)
        thread.start()
    
    def _setup_logging(self, app):
        """
        设置结构化日志
        """
        import structlog
        
        # 配置structlog
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )
    
    @staticmethod
    def update_active_users(count: int):
        """
        更新活跃用户数
        """
        ACTIVE_USERS.set(count)
```

### 1.3 Prometheus配置文件

**monitoring/prometheus/prometheus.yml**：
```yaml
# Prometheus配置文件
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# 告警规则文件
rule_files:
  - "rules/*.yml"

# 抓取配置
scrape_configs:
  # Prometheus自身监控
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  # FastAPI应用监控
  - job_name: 'hr-scheduling-api'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  # 系统监控
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
  
  # Redis监控
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
  
  # PostgreSQL监控
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

# 告警管理器配置
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

**告警规则配置**：
```yaml
# monitoring/prometheus/rules/alerts.yml
groups:
  - name: hr-scheduling-alerts
    rules:
      # 高错误率告警
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "应用错误率过高"
          description: "应用在过去5分钟内的错误率超过5%"
      
      # 响应时间告警
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "应用响应时间过长"
          description: "95%的请求响应时间超过2秒"
      
      # 内存使用告警
      - alert: HighMemoryUsage
        expr: |
          (system_memory_usage_bytes / (1024*1024*1024)) > 8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "系统内存使用过高"
          description: "系统内存使用超过8GB"
```

## 第二步：Redis缓存系统设计

### 2.1 Redis缓存架构

**缓存策略设计**：
```
应用层 → 缓存层(Redis) → 数据库层(PostgreSQL)
   ↓         ↓              ↓
业务逻辑   热点数据        持久化数据
```

**缓存配置**：
```python
# app/utils/cache.py
"""
Redis缓存管理

提供统一的缓存接口和策略管理。
"""
import redis
import json
import pickle
from typing import Any, Optional, Union
from functools import wraps
from fastapi import Depends
import logging

logger = logging.getLogger(__name__)

class CacheManager:
    """
    缓存管理器
    
    提供Redis缓存的统一接口。
    """
    
    def __init__(self, redis_url: str = None):
        self.redis_client = None
        if redis_url:
            self.init_redis(redis_url)
    
    def init_redis(self, redis_url: str):
        """
        初始化Redis连接
        """
        import os
        redis_config = {
            'host': os.getenv('REDIS_HOST', 'localhost'),
            'port': int(os.getenv('REDIS_PORT', 6379)),
            'db': int(os.getenv('REDIS_DB', 0)),
            'password': os.getenv('REDIS_PASSWORD'),
            'decode_responses': True,
            'socket_connect_timeout': 5,
            'socket_timeout': 5,
            'retry_on_timeout': True,
            'health_check_interval': 30
        }
        
        self.redis_client = redis.Redis(**redis_config)
        
        # 测试连接
        try:
            self.redis_client.ping()
            logger.info('Redis连接成功')
        except redis.ConnectionError:
            logger.error('Redis连接失败')
    
    def get(self, key: str, default=None) -> Any:
        """
        获取缓存值
        """
        try:
            value = self.redis_client.get(key)
            if value is None:
                return default
            
            # 尝试JSON解析
            try:
                return json.loads(value)
            except (json.JSONDecodeError, TypeError):
                # 如果JSON解析失败，尝试pickle
                try:
                    return pickle.loads(value.encode('latin1'))
                except:
                    return value
        except Exception as e:
            logger.error(f'缓存获取失败 {key}: {e}')
            return default
    
    def set(self, key: str, value: Any, expire: int = 3600) -> bool:
        """
        设置缓存值
        
        Args:
            key: 缓存键
            value: 缓存值
            expire: 过期时间(秒)
        """
        try:
            # 序列化值
            if isinstance(value, (dict, list)):
                serialized_value = json.dumps(value, ensure_ascii=False)
            elif isinstance(value, (str, int, float, bool)):
                serialized_value = json.dumps(value)
            else:
                serialized_value = pickle.dumps(value).decode('latin1')
            
            return self.redis_client.setex(key, expire, serialized_value)
        except Exception as e:
            logger.error(f'缓存设置失败 {key}: {e}')
            return False
    
    def delete(self, key: str) -> bool:
        """
        删除缓存
        """
        try:
            return bool(self.redis_client.delete(key))
        except Exception as e:
            logger.error(f'缓存删除失败 {key}: {e}')
            return False
    
    def exists(self, key: str) -> bool:
        """
        检查缓存是否存在
        """
        try:
            return bool(self.redis_client.exists(key))
        except Exception as e:
            logger.error(f'缓存检查失败 {key}: {e}')
            return False
    
    def clear_pattern(self, pattern: str) -> int:
        """
        清除匹配模式的缓存
        """
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            logger.error(f'批量缓存清除失败 {pattern}: {e}')
            return 0

# 全局缓存实例
cache = CacheManager()

def cached(expire: int = 3600, key_prefix: str = ''):
    """
    缓存装饰器
    
    Args:
        expire: 过期时间(秒)
        key_prefix: 缓存键前缀
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # 生成缓存键
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # 尝试从缓存获取
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                logger.debug(f'缓存命中: {cache_key}')
                return cached_result
            
            # 执行函数并缓存结果
            result = func(*args, **kwargs)
            cache.set(cache_key, result, expire)
            logger.debug(f'缓存设置: {cache_key}')
            
            return result
        return wrapper
    return decorator
```

### 2.2 会话管理配置

**Flask-Session配置**：
```python
# config/development.py
class DevelopmentConfig:
    # Redis会话配置
    SESSION_TYPE = 'redis'
    SESSION_REDIS = redis.from_url('redis://localhost:6379/1')
    SESSION_PERMANENT = False
    SESSION_USE_SIGNER = True
    SESSION_KEY_PREFIX = 'hr_session:'
    SESSION_COOKIE_NAME = 'hr_session'
    SESSION_COOKIE_DOMAIN = None
    SESSION_COOKIE_PATH = '/'
    SESSION_COOKIE_HTTPONLY = True
    SESSION_COOKIE_SECURE = False  # 生产环境设为True
    SESSION_COOKIE_SAMESITE = 'Lax'
    PERMANENT_SESSION_LIFETIME = timedelta(hours=8)
```

## 第三步：API文档自动生成

### 3.1 Swagger/OpenAPI配置

**安装文档生成依赖**：
```txt
# API文档
flask-restx==1.2.0
flasgger==0.9.7.1
marshmallow-dataclass==8.6.0
```

**API文档配置**：
```python
# app/utils/swagger.py
"""
Swagger API文档配置

自动生成API文档和交互式测试界面。
"""
from flask_restx import Api, Resource, fields
from flask import Blueprint

# 创建API蓝图
api_bp = Blueprint('api', __name__, url_prefix='/api')

# 配置API文档
api = Api(
    api_bp,
    version='1.0',
    title='人力资源调度系统 API',
    description='人力资源调度系统后端API文档',
    doc='/docs/',
    prefix='/api',
    contact='开发团队',
    contact_email='dev@company.com'
)

# 定义通用响应模型
api_response = api.model('APIResponse', {
    'success': fields.Boolean(required=True, description='请求是否成功'),
    'message': fields.String(required=True, description='响应消息'),
    'data': fields.Raw(description='响应数据'),
    'timestamp': fields.DateTime(description='响应时间')
})

# 定义分页响应模型
pagination_response = api.model('PaginationResponse', {
    'success': fields.Boolean(required=True, description='请求是否成功'),
    'message': fields.String(required=True, description='响应消息'),
    'data': fields.Raw(description='响应数据'),
    'pagination': fields.Nested(api.model('Pagination', {
        'page': fields.Integer(description='当前页码'),
        'per_page': fields.Integer(description='每页数量'),
        'total': fields.Integer(description='总记录数'),
        'pages': fields.Integer(description='总页数')
    })),
    'timestamp': fields.DateTime(description='响应时间')
})

# 定义用户模型
user_model = api.model('User', {
    'id': fields.Integer(required=True, description='用户ID'),
    'username': fields.String(required=True, description='用户名'),
    'email': fields.String(required=True, description='邮箱'),
    'full_name': fields.String(description='全名'),
    'role': fields.String(description='角色'),
    'is_active': fields.Boolean(description='是否激活'),
    'created_at': fields.DateTime(description='创建时间'),
    'updated_at': fields.DateTime(description='更新时间')
})

# 定义认证模型
login_model = api.model('Login', {
    'identifier': fields.String(required=True, description='用户名或邮箱'),
    'password': fields.String(required=True, description='密码'),
    'remember_me': fields.Boolean(description='记住我')
})

token_response = api.model('TokenResponse', {
    'access_token': fields.String(required=True, description='访问令牌'),
    'refresh_token': fields.String(required=True, description='刷新令牌'),
    'expires_in': fields.Integer(required=True, description='过期时间(秒)'),
    'token_type': fields.String(required=True, description='令牌类型')
})
```

**用户API文档示例**：
```python
# app/api/users.py
from flask_restx import Resource, fields
from app.utils.swagger import api, user_model, api_response, pagination_response
from app.auth.decorators import jwt_required, admin_required

ns = api.namespace('users', description='用户管理')

@ns.route('/')
class UserListAPI(Resource):
    @ns.doc('list_users')
    @ns.param('page', '页码', type='integer', default=1)
    @ns.param('per_page', '每页数量', type='integer', default=20)
    @ns.param('search', '搜索关键词', type='string')
    @ns.marshal_with(pagination_response)
    @jwt_required
    def get(self):
        """
        获取用户列表
        
        支持分页和搜索功能。
        """
        # 实现逻辑...
        pass
    
    @ns.doc('create_user')
    @ns.expect(user_model)
    @ns.marshal_with(api_response)
    @admin_required
    def post(self):
        """
        创建新用户
        
        只有管理员可以创建用户。
        """
        # 实现逻辑...
        pass

@ns.route('/<int:user_id>')
class UserAPI(Resource):
    @ns.doc('get_user')
    @ns.marshal_with(api_response)
    @jwt_required
    def get(self, user_id):
        """
        获取用户详情
        
        根据用户ID获取用户详细信息。
        """
        # 实现逻辑...
        pass
```

## 第四步：Docker容器化部署

### 4.1 Docker配置文件

**应用Dockerfile**：
```dockerfile
# Dockerfile
FROM python:3.11-slim

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 创建非root用户
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# 暴露端口
EXPOSE 5000

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# 启动命令
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "--timeout", "120", "run:app"]
```

**Docker Compose配置**：
```yaml
# docker-compose.yml
version: '3.8'

services:
  # 应用服务
  app:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://postgres:password@db:5432/hr_scheduling
      - REDIS_URL=redis://redis:6379/0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # 数据库服务
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=hr_scheduling
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
  
  # Redis服务
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes
  
  # Nginx反向代理
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped
  
  # Prometheus监控
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped
  
  # Grafana仪表板
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

### 4.2 Nginx配置

**nginx/nginx.conf**：
```nginx
events {
    worker_connections 1024;
}

http {
    upstream app_servers {
        server app:5000;
        # 可以添加多个应用实例实现负载均衡
        # server app2:5000;
    }
    
    # 限流配置
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    
    server {
        listen 80;
        server_name localhost;
        
        # 重定向HTTP到HTTPS
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name localhost;
        
        # SSL配置
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        
        # 安全头
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
        
        # API代理
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://app_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 超时配置
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }
        
        # 静态文件
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
        
        # 健康检查
        location /health {
            proxy_pass http://app_servers;
            access_log off;
        }
    }
}
```

## 第五步：CI/CD自动化部署

### 5.1 GitHub Actions配置

**.github/workflows/deploy.yml**：
```yaml
name: Deploy to Production

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        FLASK_ENV: testing
      run: |
        pytest --cov=app --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
    
    - name: Security scan
      run: |
        bandit -r app/
        safety check
  
  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          mycompany/hr-scheduling:latest
          mycompany/hr-scheduling:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PRODUCTION_HOST }}
        username: ${{ secrets.PRODUCTION_USER }}
        key: ${{ secrets.PRODUCTION_SSH_KEY }}
        script: |
          cd /opt/hr-scheduling
          docker-compose pull
          docker-compose up -d --remove-orphans
          docker system prune -f
```

## 学习检查点和代码规范

### 生产环境检查清单

**安全检查**：
- [ ] HTTPS配置和SSL证书
- [ ] 环境变量和密钥管理
- [ ] API限流和防护
- [ ] 数据库连接加密
- [ ] 日志脱敏处理

**性能检查**：
- [ ] 数据库索引优化
- [ ] 缓存策略配置
- [ ] 静态资源CDN
- [ ] 负载均衡配置
- [ ] 监控告警设置

**可靠性检查**：
- [ ] 健康检查端点
- [ ] 优雅关闭处理
- [ ] 数据备份策略
- [ ] 故障恢复流程
- [ ] 滚动更新配置

### 运维命令参考

**Docker操作**：
```bash
# 构建和启动服务
docker-compose up -d

# 查看服务状态
docker-compose ps

# 查看日志
docker-compose logs -f app

# 扩展应用实例
docker-compose up -d --scale app=3

# 滚动更新
docker-compose pull && docker-compose up -d
```

**监控命令**：
```bash
# 查看系统资源
docker stats

# 查看应用指标
curl http://localhost:5000/metrics

# 数据库连接检查
docker-compose exec db psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"
```

## 能力评估标准

**专家水平** (完成本阶段后应达到):
- [ ] 能够设计和实现完整的监控体系
- [ ] 掌握Redis缓存策略和性能优化
- [ ] 熟练使用Docker进行容器化部署
- [ ] 能够配置负载均衡和高可用架构
- [ ] 掌握CI/CD流水线的设计和实现
- [ ] 理解生产环境的安全和性能要求
- [ ] 能够进行故障排查和性能调优

**评估方式**:
1. 完成完整的生产环境部署
2. 实现监控告警和自动化运维
3. 通过压力测试和安全扫描
4. 能够处理生产环境故障

## 总结

恭喜您完成了Python后端开发的完整学习计划！通过四个阶段的学习，您已经掌握了：

1. **项目初始化**：标准化的项目结构和开发环境
2. **基础开发**：Flask框架和RESTful API开发
3. **进阶功能**：认证授权、测试和日志系统
4. **项目优化**：监控、缓存、文档和容器化部署

现在您具备了开发和部署企业级Python后端应用的完整技能栈，可以独立承担复杂的后端开发项目。

---

*学习计划完成！继续实践和深入学习，成为Python后端开发专家。*